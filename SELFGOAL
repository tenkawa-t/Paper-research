SELFGOAL: Your Language Agents Already Know
How to Achieve High-level Goals


はい、高レベルの目標や複雑なタスクを解決するための類似のアプローチがいくつか存在します。SELFGOALと同様の課題に取り組んでいる研究をいくつか紹介します：

OKR-Agent (Zheng et al., 2023):

目標と主要な結果（OKR）フレームワークを使用
階層的な自己協調と自己修正メカニズムを採用
複雑なタスクを管理するために複数のサブエージェントを使用


ADAPT (Prasad et al., 2024):

複雑なタスクを再帰的に分解
フィードバックに基づいて目標を動的に再分解
言語モデルを使用して計画と実行を行う


Reflexion (Shinn et al., 2023):

失敗したタスクの試行を振り返り、新しい計画を立てる
言語による強化学習アプローチを採用


CLIN (Majumder et al., 2023):

過去の経験から因果関係の抽象的な記憶を作成
連続的に学習し、新しいタスクに適応


Decomposed Prompting (Khot et al., 2022):

複雑なタスクを共有プロンプトのライブラリに分解
マルチステップ推論タスクに対して効果的


Plan-and-Solve Prompting (Wang et al., 2023):

タスクを計画と解決のステップに分割
ゼロショット学習でチェーン・オブ・ソート推論を改善



これらのアプローチはそれぞれ異なる方法で高レベルの目標や複雑なタスクに取り組んでいますが、共通して以下の要素に焦点を当てています：

タスクの分解
動的な計画立案
フィードバックや経験からの学習
階層的なアプローチ
言語モデルの能力の活用

SELFGOALは、これらのアプローチの多くの要素を組み合わせつつ、特に動的な環境での適応と高レベル目標の達成に焦点を当てている点が特徴的です。

要約いたします。

この研究は、大規模言語モデル（LLM）を基にした言語エージェントが高レベルの目標を効果的に達成するための新しい方法「SELFGOAL」を提案しています。

主なポイント：

1. 背景：LLMベースのエージェントは複雑なタスクを解決できますが、詳細な指示なしで高レベルの目標を達成することや、フィードバックが遅れる環境に適応することに課題があります。

2. SELFGOALの概要：
   - 高レベルの目標を実用的なサブゴールのツリー構造（GOALTREE）に分解します。
   - 環境との相互作用中にこの構造を動的に更新し、最も有用なサブゴールを特定します。

3. 主要な特徴：
   - 検索モジュール：現在の状況に最適なサブゴールを選択します。
   - 分解モジュール：選択されたサブゴールをさらに具体的なサブゴールに分解します。
   - 行動モジュール：選択されたサブゴールを指針として使用し、行動を決定します。

4. 実験結果：
   - 競争的、協調的、遅延フィードバック環境を含む様々なタスクで評価されました。
   - SELFGOALは他の方法（ReAct、ADAPT、Reflexion、CLIN）と比較して優れたパフォーマンスを示しました。
   - 特に小規模なLLMでも効果を発揮し、一貫した改善が見られました。

5. 利点：
   - 詳細な指示なしで高レベルの目標を達成する能力を向上させます。
   - 動的な環境に適応し、長期的な目標を追求できます。
   - 既存の知識と環境からのフィードバックを効果的に組み合わせます。

6. 将来の課題：
   - モデルの理解力と要約能力に依存する面があり、小規模モデルでの効果に制限がある可能性があります。

結論として、SELFGOALは言語エージェントの能力を大幅に向上させ、複雑な環境での高レベルの目標達成を可能にする有望なアプローチであることが示されました。



はい、承知しました。まずは抄録（Abstract）から翻訳を始めます。

1. 抄録（Abstract）の翻訳：

大規模言語モデル（LLM）を活用した言語エージェントは、ゲームやプログラミングなどの分野における意思決定ツールとしてますます価値が高まっています。しかし、これらのエージェントは、詳細な指示がない状態で高レベルの目標を達成することや、フィードバックが遅れる環境に適応することに課題を抱えています。本論文では、SELFGOAL という新しい自動アプローチを提示します。これは、人間からの事前知識や環境からのフィードバックが限られた状況下で、エージェントが高レベルの目標を達成する能力を向上させるように設計されています。SELFGOALの核心的概念は、環境との相互作用中に高レベルの目標をより実用的なサブゴールのツリー構造に適応的に分解し、最も有用なサブゴールを特定してこの構造を徐々に更新することです。実験結果は、SELFGOALが競争的、協調的、遅延フィードバック環境を含む様々なタスクにおいて、言語エージェントのパフォーマンスを大幅に向上させることを示しています。


2. はじめに（Introduction）の翻訳：

大規模言語モデル（LLM）の進歩により、タスク特有のトレーニングなしで動的環境における複雑なタスクを解決する自律型言語エージェント（またはLLMベースのエージェント）の構築が可能になりました。現実には、これらの自律型エージェントは、「最大の利益を得る」や「競争に勝つ」といった非常に広範で高レベルの目標を与えられることが多く、その曖昧な性質と遅延報酬が自律的なタスク解決に大きな課題をもたらします。さらに重要なことに、新しい目標やタスクに適応するためにこれらのモデルを頻繁にトレーニングすることは現実的ではありません。したがって、重要な問題が浮上します：トレーニングなしで自律型言語エージェントが一貫して高レベルの目標を達成できるようにするにはどうすればよいでしょうか？

先行研究では、言語エージェントが高レベルの目標を達成するための指示に二種類の補助的ガイダンスを作成することに焦点を当てています：事前のタスク分解と事後の経験要約です。前者は、行動前にタスクを分解し、LLMの事前知識を利用して高レベルの目標を、手元の特定の行動に関連するより具体的なサブゴールに分解することを含みます。しかし、この研究の方向性は、相互作用中にこれらのサブゴールを環境に根付かせることがないため、経験的なガイダンスが失われる結果となっています。対照的に、後者はエージェントが環境と直接相互作用し、履歴から貴重な経験を要約することを可能にします（例：「XはYに貢献する」）。しかし、経験からルールを導き出すことの困難さにより、ガイダンスが単純で構造化されていないため、戦略を効果的に優先順位付けしたり調整したりすることが難しくなっています。

両者の長所を組み合わせる自然な解決策は、環境との相互作用中にタスクとその高レベルの目標を動的に分解することです。このアプローチでは、エージェントが詳細さと側面が異なるガイドラインを構築し使用する必要があります。ツリー構造はこの要件に理想的です。階層的な組織を可能にし、必要に応じて広範な概要と詳細なガイダンスの両方を提供するためです。しかし、このアプローチには2つの大きな課題があります：1）タスク実行中、すべてのノードが現在の文脈に関連しているわけではないため、現在の行動を導くのに最も適したノードを選択する必要があります。例えば、予算が厳しい場合、「最も高価な品目に入札する」よりも「値引きを注視する」方が賢明な選択です。2）ノードが提供するガイダンスの粒度はツリーの深さとともに増加しますが、適切な詳細レベルはシナリオによって異なるため、固定されたツリーの深さは一般的ではありません。例えば、「より多くのお金を稼ぐ」といった一般的なガイドラインはオークションでは役に立ちません。

これらの課題に取り組むため、我々はSELFGOALを提案します。これは、言語エージェントが事前知識と環境からのフィードバックの両方を活用して高レベルの目標を達成するための自己適応型フレームワークです。主なアイデアは、テキストによるサブゴールのツリーを構築し、エージェントが状況に基づいて適切なものをプロンプトのガイドラインとして選択することです。


はい、続きを翻訳いたします。

具体的には、図1に示すように、SELFGOALはタスク実行中にGOALTREEを構築、更新、活用するための2つの主要モジュールを特徴としています：

1) 検索モジュール：提供された現在の状態とGOALTREEの既存ノードに基づいて、最も適したゴールノードの上位K個を選択するようプロンプトされます。これはLLMの事前知識を活用します。

2) 分解モジュール：ゴールノードをより具体的なサブゴールのリストに分解し、後続の葉ノードとすることで、GOALTREEの適応的な自己成長を確保します。なお、新しいノードと既存のゴールノードとのテキスト類似性に基づいて、分解中に冗長なノードを除外していることに注意してください。

3) 行動モジュール：選択されたサブゴールをガイドラインとして入力とし、現在の状態に対する行動をLLMにプロンプトします。

様々な競争および協力シナリオでの広範な実験により、SELFGOALが高レベルの目標に対して正確なガイダンスを提供し、多様な環境に適応することで、言語エージェントのパフォーマンスを大幅に向上させることが示されています。

要約すると、本論文の貢献は以下の通りです：

• 頻繁な再トレーニングの必要なしに、自律型言語エージェントが一貫して高レベルの目標を達成できるようにするという課題に取り組んでいます。

• SELFGOALを導入しています。これは、環境との相互作用中にタスクの高レベルの目標をサブゴールに動的に分解するGOALTREEを構築、更新、活用する自己適応型フレームワークです。

• エージェントが目標から逸脱しがちな協調的および競争的シナリオの両方で広範な実験を行っています。結果は、SELFGOALが言語エージェントの高レベルの目標に一貫して従う能力を大幅に向上させることを示しています。


承知しました。次のセクションは「関連研究（Related Work）」です。このセクションを翻訳いたします。

関連研究（Related Work）：

フィードバックからの学習：
LLMは目標指向の言語エージェントを構築するための有望なツールとなっています。世界の状態、タスク、相互作用の履歴を含むテキスト入力により、言語エージェントは目標を達成するための次の行動を決定します。研究では、環境からのフィードバックを通じて言語エージェントの推論と計画能力を強化することが探求されています。例えば、Reflexionはエージェントが失敗を振り返り、過去の間違いを考慮した新しい計画を立てることを可能にします。同様に、VoyagerはMinecraftで動作し、失敗に関する詳細なフィードバックからコードベースのスキルライブラリを開発します。最近の研究では、失敗と成功の両方の試みを分析し、因果関係の抽象的な記憶をまとめています。しかし、フィードバックから直接学習されたものは往々にして一般的すぎて体系的ではなく、戦略を効果的に優先順位付けすることが難しくなっています。

意思決定のためのLLM：
LLMはロボット工学、テキストゲーム、社会的タスクなどの対話型環境での意思決定のためのポリシーモデルとしてますます使用されています。しかし、これらの環境での目標は、ScienceWorldの「果物を見つける」のように、単純で具体的なことが多いです。長期的で高レベルの目標に対しては、LLMは効果的に機能することが難しく、追加のモジュールによるサポートが必要です。我々の研究では、LLMのパラメータを更新する必要のない方法を使用し、言語エージェントが環境との相互作用中に一貫して高レベルの目標を追求できるようにします。

分解とモジュール性：
複雑な意思決定タスクをサブタスクに分解することは、LLMのタスク解決能力を向上させる従来の方法です。階層的タスクネットワークのようなアプローチは、手動で指定された計画のライブラリを含むドメイン知識を活用して、複雑な問題を簡素化します。最近の研究では、LLMに目標を分解する役割を割り当てています。例えば、Decomposed Promptingは、マルチステップ推論タスクを共有プロンプトライブラリに分解するために少数ショットプロンプティングアプローチを使用します。OKR-Agentは、階層的エージェントによってサポートされる自己協調と自己修正メカニズムを利用してタスクの複雑さを管理します。ADAPTは、意思決定タスクでフィードバックに基づいて目標を再帰的に再分解することをLLMに可能にします。しかし、これらのアプローチは多くの場合、環境との相互作用前にタスクを分解するため、根拠のある動的な調整が欠けています。これに対処するため、我々はモジュール式の目標分解と環境フィードバックからの学習を組み合わせることを目指しています。

はい、続きを翻訳いたします。次のセクションは「方法論（Methodology）」です。

方法論（Methodology）：

人間が複雑なタスクを高レベルの目標（例：「将来の株価を予測する」）で実行する際、通常はそれを具体的な詳細サブゴール（例：「過去の価格データを収集し、最近の市場イベントに基づいて予測を調整する」）に分解して効果的に実行します。この考えに触発され、本論文ではSELFGOALを提案します。これは、言語エージェントが高レベルの目標を活用し達成するためのノンパラメトリック学習アプローチです。SELFGOALは、意思決定に有用なガイダンスを表すノードのツリーを用いて、高レベルの目標のトップダウン階層的分解を行います。

このセクションでは、まずSELFGOALの動作概要を§3.1で説明します。次に、タスク実行を導くGOALTREEを維持するSELFGOALの3つの主要モジュール（検索、分解、行動）の詳細を§3.2で説明します。

3.1 SELFGOALの概要

問題の定式化：高レベルの目標を持つタスク
まず、エージェントが動的な環境と相互作用し、高レベルの目標の達成に基づいて評価される我々の研究対象タスクの特徴を定式化します。アクターモデルM_aが環境Eとの相互作用を通じて高レベルの目標g_0を達成することを目指すシナリオに焦点を当てます。M_aが採用するポリシーはπ_θと表記します。各タイムステップtでπ_θは行動a_tを生成し、環境Eは状態s_tを返します。この行動-状態ペア{a_t, s_t}はπ_θを更新するために使用されます。SELFGOALは、常に即時の報酬を持たない長期的なタスクの達成もサポートすることに注意してください。この場合、M_aはタスクを完了することによってのみ、目標g_0の達成度に応じてスコアで評価されます。

SELFGOALのワークフロー
SELFGOALは言語エージェントのためのノンパラメトリック学習アルゴリズム、つまりパラメータ更新を伴わないアルゴリズムです。SELFGOALのワークフローはアルゴリズム1に示されており、π_θ = pを設定し、pをM_a（LLM）に提供される指示プロンプトとしてモデル化します。つまり、a_t = LLM(p_t, s_{t-1})となります。ポリシーπ_θは、現在の状況に最も適したサブゴール指示g_{i,j}（i層目、j番目の葉ノード）の修正である、pの修正を通じて更新されます。具体的に、SELFGOALは2つの主要モジュール、分解と検索を特徴としており、これらはそれぞれサブゴールツリーTを構築および利用して環境と相互作用します。このツリーをGOALTREEと呼びます。タスクの高レベルの目標をGOALTREEのルートノードとして設定し、検索モジュールは現状に役立つノードを見つけ、分解モジュールはそのノードが十分に明確でない場合、サブゴールを葉ノードとして分解します。


はい、続けて翻訳いたします。

3.2 SELFGOALの詳細

検索：現在の状況に適したサブゴールの特定
SELFGOALの検索モジュールでは、エージェントのバックボーンLLMに、現在の状況に最も適切なサブゴールを特定するよう求めます。例えば、「現在の状況であなたのメイン目標に到達するのに役立つ最も有用なK個のサブゴールを選択してください...」（完全なプロンプトは付録A.4を参照）というようにします。タイムステップtにおける現在の状態s_tを、環境との対話履歴の説明として表現します。また、GOALTREEの各枝の葉ノードを候補サブゴールリストとして、LLMに最も適切なものを決定させます。LLMはその後、最も適切なK個のサブゴールを選択し、分解を行い、このステップでの指示プロンプトp_tを更新します。

分解：環境に適応するためのGOALTREEの改良
現在の行動-状態ペア{a_t, s_t}に基づいて、GOALTREEは、エージェントに有用なガイダンスを提供するのに十分具体的でない場合、分解を通じて更新されます。検索モジュールで選択されたサブゴールg_{i,j}（最初はg_0に設定）をバックボーンLLMを使って分解します。「{a_t, s_t}に基づいて、{g_{i,j}}からどのようなサブゴールを導き出せますか」というような指示でLLMにプロンプトを与え、新しいサブゴールの集合Gを生成します（付録A.4も参照）。これらのサブゴールの粒度を制御するために、新しいサブゴールと既存のサブゴールとのコサイン類似度がξを超える場合、現在のノードは更新されないフィルタリングメカニズムを適用します。そうでない場合は、新しいサブゴールを現在のノードの下に追加し、GOALTREEを拡張します。さらに、N回連続でGOALTREEに新しいノードが追加されない場合、更新を停止する停止メカニズムを設計しています。

行動：サブゴールを利用して行動を取る
SELFGOALによって有用と判断されたGOALTREEからのサブゴールを取得した後、エージェントはLLMの指示プロンプトp_tを更新し、環境と相互作用するための行動a_tを取ります。このステップのプロンプトも付録A.4で見ることができます。

4 実験設定

4.1 タスクと環境

我々は、SELFGOALを4つの動的タスクで評価しています。これらのタスクには高レベルの目標があり、公共財ゲーム、平均の2/3を推測するゲーム、第一価格オークション、交渉が含まれます。これらは既存の研究によって実装されています。表1に示すように、これらは単一ラウンドまたは複数ラウンドのゲームであり、複数のエージェントの協力または競争を必要とします。複数ラウンドのゲームでは、エージェントはゲームの終了時にのみ遅延報酬を受け取ることに注意してください。我々の実験では、安定した結果を得るために単一ラウンドのゲームをT=20回、複数ラウンドのゲームをT=10回繰り返します。

はい、続けて翻訳いたします。

公共財ゲーム：GAMA-Bench
このゲームの実装環境としてGAMA-Benchを使用します。具体的には、N=5人のプレイヤーがそれぞれ個別に公共のポットに寄付するトークンの数を決定します。ポット内のトークンは係数R（1 ≤ R ≤ N）で乗算され、作成された「公共財」は全プレイヤーに均等に分配されます。プレイヤーは寄付しなかったトークンを保持します。簡単な計算により、プレイヤーが1トークン寄付するごとに、その純利益は R/N - 1（収入 - 寄付）となることがわかります。この値が負であることから、各プレイヤーにとって最も合理的な戦略はトークンを全く寄付しないことであることが示唆されます。この戦略はゲームにおいてナッシュ均衡をもたらします。同じバックボーンモデルを使用し、同じ方法（例：CLINやSELFGOAL）を装備したNエージェントが互いにゲームをプレイし、グループ行動を観察します。GAMA-Benchに従い、R = 2に設定しています。

平均の2/3を推測するゲーム：GAMA-Bench
GAMA-Benchの実装を使用し、Nプレイヤーが独立して0から100の間の数字を選択し、グループ平均の3分の2に最も近い数字を選んだプレイヤーが勝利します。この設定は、プレイヤーの心の理論（ToM）能力を効果的にテストします。行動経済学では、認知階層モデルがプレイヤーを以下のように分類します：レベル0のプレイヤーはランダムに数字を選びます。レベル1のプレイヤーは他のプレイヤーがレベル0であると仮定し、期待される平均50の3分の2を選びます。レベルkのプレイヤーは、参加者にレベル0からk-1が含まれると信じ、したがって(2/3)^k × 50を選びます。全プレイヤーが0を選ぶことが最適な結果であり、ナッシュ均衡を達成します。このゲームでは、同じバックボーンモデルを使用し、同じプロンプト方法（例：SELFGOAL）を装備したN=5エージェントが互いにゲームをプレイし、グループ行動を観察します。

第一価格オークション：AucArena
第一価格オークションの実装としてAucArenaを使用します。オークショニアは全参加者の入札を収集し発表し、現在の最高入札額を公開します。参加者は入札を個別に検討した後、公に決定を下す必要があります。オークションはK=15個のアイテムで構成され、各アイテムの価値は$2,000から$10,000の範囲で、アイテム間の増分は$2,000です。これらのアイテムはランダムな順序で提示され、オークションはK=15ラウンド続きます。N=4エージェントが入札者としてオークションに参加します。各エージェントはオークション終了時に最高の利益を確保し、全ての競合相手を上回ることを目指します。我々の実験では、各入札者の予算を$20,000に設定しています。様々な方法（例：SELFGOAL）で強化されたエージェントが、異なるバックボーンモデルを使用して、同じモデル（GPT-3.5）で駆動される3つの同一の対戦相手と競争します。

はい、続けて翻訳いたします。

交渉：DealOrNotDeal
複数の問題に関する交渉を実装するためにDealOrNotDealを使用します。N=2のエージェント、すなわちAliceとBobに、アイテムのセット（例：本、帽子、ボール）が提示され、その分配について交渉する必要があります。各エージェントには、各アイテムに対して0から10の間のランダムな整数値が割り当てられ、任意のエージェントにとって全アイテムの合計価値が10を超えないようにします。交渉はK=10ラウンド続き、10ラウンド以内にアイテムの分配について合意に達しない場合、両者とも利益を得られません。目標は、二人のエージェント間の利益の差を最小化することです。AliceとBobがM=50個のアイテムについて交渉するようにランダムに選択します。交渉終了時のAliceとBobの最終利益をそれぞれP_AliceとP_Bobと定義します。

なお、Alice側のエージェントのプロンプト方法を変更し、Bob側は固定（GPT-3.5）のままにしています。

4.2 エージェントフレームワークのベースラインとバックボーンLLM

上記のタスクにおいて高レベルの目標を達成するためのガイダンスを提供する2種類のエージェントフレームワークを採用しています。

一つはタスク分解フレームワークで、ReActとADAPTが含まれます。ReActはエージェントが行動する前に推論することを可能にし、ADAPTはLLMが実行できない場合に複雑なサブタスクを再帰的に計画し分解します。

もう一つは経験要約フレームワークで、ReflexionとCLINが含まれます。Reflexionはエージェントに失敗したタスクの試行を反省し再試行するよう促します。CLINは過去の経験を反省することで、将来の試行を支援するための因果関係の抽象的な記憶を作成し、「AはBに必要[かもしれない/であるべき]」として表現します。

これらの言語エージェントフレームワークを駆動するために、以下のLLMを使用しています：GPT-3.5-Turbo（gpt-3.5-turbo-1106）とGPT-4-Turbo（gpt-4-1106-preview）; Gemini 1.0 Pro; Mistral-7B-Instruct-v0.2と専門家の混合（MoE）モデルMixtral-8x7B-Instruct-v0.1; Qwen 1.5（7Bと72Bのバリアント）。ランダム性を最小限に抑えるために、温度は0に設定しています。

4.3 タスクの評価指標

GAMA-Benchの公共財ゲームでは、N人のプレイヤーがT回繰り返し参加する場合、このゲームのスコアS_1は以下のように与えられます：S_1 = (1 / (N * T)) * Σ_ij C_i,j、ここでC_i,jは[0, 1]の範囲内のプレイヤーiのラウンドjにおける提案された寄付です。

GAMA-Benchの平均の2/3を推測するゲームでは、スコアS_2は以下のように計算されます：S_2 = 100 - (1 / (N * T)) * Σ_ij C_i,j、ここでC_i,jはプレイヤーiがラウンドjで選んだ数字です。


はい、続けて翻訳いたします。

AucArenaの第一価格オークションでは、エージェントの利益をランク付けするためにTrueSkillスコアを使用します（付録A.3参照）。TrueSkillスコアはベイズ統計を通じて動的なスキルレベル（μ）を推定し、同時に真のスキルの不確実性（σ）を考慮します。したがって、エージェントのパフォーマンススコアはS_3 = TrueSkillスコアとして定義されます。この方法は、オンラインゲームやトーナメントなどの競争で一般的に使用されています。

DealOrNotDealの交渉ゲームでは、利益の絶対差を計算します：S_4 = |P_Alice - P_Bob| / M、ここでP_Alice、P_Bobは交渉終了時の利益を表し、Mは交渉対象のアイテム数です。（S_4は便宜上TrueSkillスコアで表現することもできます。）

5 結果と分析

5.1 主要な結果

4つのシナリオにわたる主要な結果を表2に示します。全体として、SELFGOALは高レベルの目標を含む様々な環境で、すべてのベースラインフレームワークを大幅に上回るパフォーマンスを示しており、より大規模なLLMがより高い利益をもたらしています。生成されたガイドラインと対応するエージェントの行動を詳しく見ると、ReActやADAPTのようなタスク分解方法によって与えられたサブゴールの一部が、現在の状況にもはや適していないことがわかります。例えば、「最も高価なアイテムに入札する」は予算が厳しい場合には有用ではありません。さらに、環境との相互作用前にタスクを分解することは、実践的な経験を考慮しておらず、広範で意味のないガイダンスにつながります。例えば、公共財ゲームでは、ADAPTは「将来のラウンドのために十分なトークンを私的コレクションに保持しながら、有意な報酬を得るために公共のポットに十分なトークンを寄付することのバランスを取ることが重要です」というような広範なサブゴールを提供します。

対照的に、Reflexionや CLINなどの事後経験要約方法は、メイン目標との相関が欠け、エージェントを目標から逸脱させる可能性のある、あまりにも詳細なガイドラインを導く傾向があります。例えば、CLINは「対戦相手が選んだ数字の分布を考慮することが、自分の選択に関する十分な情報に基づいた決定を下すために必要かもしれない」というような細部に焦点を当てたサブゴールを生成します。

これに比べ、SELFGOALは両方の短所を克服しています。各ラウンドで、SELFGOALはゲームの進行に合わせて既存のガイダンスを参照しながら、メイン目標に沿った新しいノードを分解します。例えば、公共財ゲームでは、初期のサブゴールは「プレイヤーは他のプレイヤーの行動と公共のポットにおけるトークンの全体的な分配の評価に基づいて戦略的に寄付することを目指す」です。ゲーム中にすべてのプレイヤーが公共のポットへの寄付を減らした場合、SELFGOALはその観察を吸収し、既存のノードを「グループの平均寄付が最近のラウンドで増加していることに気づいた場合、プレイヤーは過剰に寄付して自分の利益を失う可能性を避けるために、現在のラウンドでより少ないトークンを寄付することを選択するかもしれない」と洗練します。新しいサブゴールを実用的なガイドラインとして、エージェントは寄付を動的に調整できます。


はい、引き続き翻訳いたします。

興味深いことに、SELFGOALは小規模なLLMでも優れたパフォーマンスを示しています。一方で、他の方法は、これらのモデルの帰納と要約能力の不足により、同様の改善を示すことができません。例えば、CLINはMistral-7Bで平均の2/3を推測するゲームにおいてReflexionよりも0.7劣り、Qwen-7Bでは5.77劣っていますが、SELFGOALは一貫して改善をもたらしています。これは、SELFGOALにおけるGOALTREEの論理的で構造化された設計に起因すると考えられます。分解の各段階で、モデルはGOALTREEの最後の層にある既存のサブゴールを明確な参照として受け取り、分解を容易にしています。

異なるエージェントフレームワーク間の競争
これまでの結果は主に固定されたベースライン（GPT-3.5）に対して評価されています。これらのエージェントフレームワークが互いに競争する際の振る舞いを理解するために、マルチエージェント比較のためのAucArenaを設定しました。表3に示すように、SELFGOALはベースラインに対して明確な優位性を持っています。入札行動をより詳しく見ると、他の方法は過度に慎重になる傾向があることがわかります。彼らはしばしば入札を停止したり、入札が始まると参加を避けたりし、結果として利益がゼロになることがあります。しかし、SELFGOALは異なるアプローチを取り、競争が激しくない入札戦の初期段階で頻繁に入札します。これにより、高優先度のアイテムを早期に購入し、後期段階での激しい競争を避けることができます。

5.2 SELFGOALの分析

GOALTREEのガイドラインの粒度はタスク解決にどのように影響するか？
§5.1で議論したように、SELFGOALは異なる深さを設定することで動的な環境に適応し、より深い層のサブゴールノードがより詳細な指示を提供します。ここでは、このような粒度がSELFGOALのパフォーマンスにどのように影響するかを探ります。オークションと交渉環境をテストベッドとして使用し、停止メカニズムの閾値ξを0.6、0.7、0.8、0.9に設定してサブゴールのレベルを変更しました。図2によると、エージェントのパフォーマンスは深さの増加とともに初期には改善しますが、最終的には低下します。浅いツリー（ξ = 0.6）はガイダンスの詳細が不足しているため、最も悪いパフォーマンスにつながります。しかし、最も深いツリー（ξ = 0.9）も優れたパフォーマンスを示さず、おそらく繰り返しのガイダンスがモデルの有用なガイダンスの選択を妨げるためだと考えられます。冗長なノードは候補セットを増やし、検索モジュールが全ての価値あるノードを選択することを難しくします。実際、検索モジュールは常に同じ意味を表す複数のノードに焦点を当て、他の有用なノードを見逃す結果となります。この実験は、より詳細な指示が言語エージェントの高レベルの目標達成を支援することを確認していますが、過度に詳細なガイダンスの欠点を緩和するために、ガイダンスツリーのバランスの取れた適応的な深さが必要であることを示しています。

はい、続けて翻訳いたします。

検索モジュールの実験的検証
SELFGOALの検索モジュールは有用なサブゴールノードを見つけることに成功していますか？我々は元のLLMベースの検索モジュール（GPT-3.5で実装）を置き換えるために2つの方法をベースラインとして採用しました。一つはランダム選択で、サブゴールノードの集合からランダムにノードを選択します。もう一つは埋め込み類似度に基づく選択で、コサイン類似度に基づいて現在の状況に最も類似したサブゴールを選択します。オークションと交渉のような複数ラウンドのゲームで、これらの方法のランキングを評価するためにTrueskillスコアを使用しています。図3に示すように、LLM検索モジュールは両方のゲームでより良いスコアを獲得しています。また、類似度ベースの方法は交渉では、ランダム選択よりも悪いパフォーマンスを示しています。これは、ガイダンスが通常短く、サブゴールと状況間の意味的な埋め込みを捉えるのが難しいためかもしれません。この実験は、SELFGOALの設計におけるLLMベースの検索モジュールの合理性を示しています。

GOALTREEの品質は目標達成にどのように影響するか？
SELFGOALに対するGOALTREEの影響を探るために、オークションと交渉ゲームで、GOALTREEを構築するモデルをGPT-4またはGPT-3.5に置き換えて比較実験を行いました。ツリーを利用するモデルはGPT-3.5に固定しています。図4の結果は、より高品質のGOALTREE（GPT-4から）がSELFGOALのパフォーマンスを大幅に向上させることを示しています。オークションで+2.87、交渉で+3.10の利得が、GPT-3.5を使用した場合と比較して得られています。この改善は、より良い理解力と要約能力を備えた強力なモデルによって生成される、より豊富で高品質なガイダンスによるものです。（GOALTREEに対する剪定の影響についての実験的検証は付録A.1で行っています。）

SELFGOALはエージェントの行動の合理性を改善できるか？
最終的なパフォーマンスの向上だけでなく、各ターンでのエージェントの行動がSELFGOALから利益を得ているかどうかにも興味があります。そこで、GAMA-Benchの2つのゲームを使用して、SELFGOALがモデルの行動に与える影響を調査しました。ここでは行動の変化がより評価しやすいためです。ここでは、SELFGOALから大きな改善を示したLLM、つまり公共財ゲームではMistral-7B、平均の2/3を推測するゲームではQwen-72Bを使用しています。20回の繰り返し実験からのデータを視覚化することで、モデルの数字予測とトークン寄付のパターンを記録しました。SELFGOALを使用すると、公共財シナリオのエージェントは、図5(a)に示すように、他の方法を使用した場合と比較して一貫してより合理的に行動しています。推測ゲームでは、強化されたモデルは図5(b)に示すように、より滑らかで着実に減少する曲線を示し、各ラウンドでナッシュ均衡により迅速に収束していることを示しています。

はい、続けて翻訳いたします。

5.3 ケーススタディ

動的な環境で異なるフレームワークのエージェントがどのように推論し計画を立てるかを示すために、小規模LLMであるMistral-7Bをバックボーンとして使用し、交渉ゲームでケーススタディを行いました（図6）。SELFGOALの粒度制御の強調が明確な利点を提供していることがわかりました。SELFGOALは「明確化の質問をする」などの実行可能なガイダンスをエージェントに提供し、エージェントに対戦相手の心理評価や各アイテムの異なる評価に早期に注意を払うよう促します。相手の評価を取得した後、SELFGOALは「譲歩する」などのガイダンスを与え、エージェントに利益の差を最小化するために特定のアイテムを諦める計画を提案するよう導きます。

対照的に、CLINはエージェントに「相手の好みを考慮する」ようアドバイスし、エージェントは相手の好みに焦点を当てますが、相手の収入を改善するために自身の利益を犠牲にする計画につながる可能性があります。事前にタスクを分解するADAPTは、「平等な割り当て」などの非常に広範なアドバイスを提供します。このような一般的なアドバイスは利益の差を最小化することを目指しますが、相手の評価に関する知識が不足しているシナリオには適していない可能性があります。結果として、モデルは相手の評価を最初に明確にすることなく割り当て計画を提案し、全ての参加者が各アイテムに対して同じ評価を持っていると仮定しています。

6 結論

本論文では、様々な動的タスクと環境にわたる高レベルの目標達成のためのLLMの能力を向上させるエージェントフレームワークSELFGOALを紹介しました。SELFGOALが環境との相互作用に基づいて文脈的なサブゴールの階層的GOALTREEを動的に生成および洗練することで、エージェントのパフォーマンスを大幅に向上させることを実証しました。実験は、この方法が競争的および協力的シナリオの両方で効果的であり、ベースラインアプローチを上回ることを示しています。さらに、SELFGOALを装備したエージェントが環境とさらに関与するにつれて、GOALTREEを継続的に更新することができ、エージェントがより高い精度と適応性で複雑な環境をナビゲートすることを可能にします。しかし、SELFGOALは小規模モデルでも効果的ですが、モデルの理解力と要約能力への需要がまだあり、これがSELFGOALが完全な効果を発揮することを妨げている可能性があることにも注意しています。

以上で論文の主要部分の翻訳が完了しました。付録や参考文献などの詳細な部分は含まれていませんが、論文の主な内容と結論を網羅しています。



